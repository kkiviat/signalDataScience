library(dplyr)
library(ggplot2)
# Load data
setwd('C:/Users/Andrew/Documents/Signal/curriculum/datasets/speed-dating-simple/')
df = read.csv('speed-dating-simple.csv')
df_attr = select(df, -intel_o, -amb_o, -fun_o, -sinc_o)
df_attr = filter(df_attr, gender==1)
df_attr = select(df_attr, -gender)
# Single train/test split
split_data = function(df) {
groups = sample(1:nrow(df)) %% 2
train = df[groups == 0, ]
test = df[groups == 1, ]
list(train=train, test=test)
}
split_predict = function(train, test) {
model = lm(attr_o ~ ., train)
pred_train = predict(model, train)
pred_test = predict(model, test)
list(train=pred_train, test=pred_test)
}
rmse = function(x, y) sqrt(mean((x-y)^2))
niter = 100
rmse_train = rep(0, niter)
rmse_test = rep(0, niter)
for (i in 1:niter) {
print(paste('Iteration:', i))
splits = split_data(df_attr)
preds = split_predict(splits$train, splits$test)
rmse_train[i] = rmse(preds$train, splits$train$attr_o)
rmse_test[i] = rmse(preds$test, splits$test$attr_o)
}
rmses = data.frame(rmse_train, rmse_test)
ggplot(rmses) + geom_histogram(aes(rmse_train), fill="blue", alpha=0.2) + geom_histogram(aes(rmse_test), fill="red", alpha=0.2)
sd(rmse_test)
sd(rmse_train)
# n-fold cross validation
nfold_cv = function(df, n_folds) {
# Make empty predictions vector
preds = numeric(nrow(df))
# Calculate the different folds
folds = sample(nrow(df)) %% n_folds + 1
for (i in 1:n_folds) {
# Get subsets of the data
df_train = df[folds != i, ]
df_test = df[folds == i, ]
# Fit linear model to training set
fit = lm(attr_o ~ ., df_train)
# Make predictions for test set
preds[folds == i] = predict(fit, df_test)
}
# Return RMSE
sqrt(mean((preds - df$attr_o)^2))
}
# Run 100 trials for calculation of RMSE
rmse_2 = numeric(100)
rmse_10 = numeric(100)
for (i in 1:100) {
print(paste('Iteration', i))
rmse_2[i] = nfold_cv(df_attr, 2)
rmse_10[i] = nfold_cv(df_attr, 10)
}
# Plot values of RMSE
df_rmse = data.frame(rmse_2, rmse_10)
ggplot(df_rmse) + geom_histogram(aes(x=rmse_2), alpha=0.3, fill="red") + geom_histogram(aes(x=rmse_10), alpha=0.3, fill="blue")
sd(rmse_2)
sd(rmse_10)
# Stepwise regression code by Michael Beese II (heavily modified)
backward_step = function(df) {
rmse_cv = c()
rmse_nocv = c()
n_removed = c()
n_removed_count = 1
while (length(colnames(df)) > 1) {
# Fit model
model = lm(attr_o ~ ., df)
# Add cross-validated RMSE estimate
rmse_cv = c(rmse_cv, nfold_cv(df, 10))
# Add RMSE for linear regression on whole dataset
rmse_nocv = c(rmse_nocv, rmse(predict(model, df), df$attr_o))
# Add # features removed
n_removed = c(n_removed, n_removed_count)
# Feature elimination
sumDF = as.data.frame(summary(model)$coefficients)
sumDF = cbind(sumDF, rownames(sumDF))
sumDF = sumDF[-1, ] # remove Intercept row
maxRow = filter(sumDF, sumDF[4] == max(sumDF[4])) # biggest p-value
# Eliminate column
df = select(df, -one_of(as.character(maxRow[[5]])))
n_removed_count = n_removed_count + 1
}
data.frame(n_removed=n_removed, rmse_cv=rmse_cv, rmse_nocv=rmse_nocv)
}
step_results = backward_step(df_attr)
library(dplyr)
library(ggplot2)
# Load data
#setwd('C:/Users/Andrew/Documents/Signal/curriculum/datasets/speed-dating-simple/')
setwd('/home/kira/R-curriculum/datasets/speed-dating-simple/')
df = read.csv('speed-dating-simple.csv')
df_attr = select(df, -intel_o, -amb_o, -fun_o, -sinc_o)
df_attr = filter(df_attr, gender==1)
df_attr = select(df_attr, -gender)
# Load data
#setwd('C:/Users/Andrew/Documents/Signal/curriculum/datasets/speed-dating-simple/')
setwd('/home/kira/R-curriculum/datasets/speed-dating-simple/')
# Load data
#setwd('C:/Users/Andrew/Documents/Signal/curriculum/datasets/speed-dating-simple/')
setwd('/home/kira/Dropbox/datascience/R-curriculum/datasets/speed-dating-simple/')
df = read.csv('speed-dating-simple.csv')
df_attr = select(df, -intel_o, -amb_o, -fun_o, -sinc_o)
df_attr = filter(df_attr, gender==1)
df_attr = select(df_attr, -gender)
split_data = function(df) {
groups = sample(1:nrow(df)) %% 2
train = df[groups == 0, ]
test = df[groups == 1, ]
list(train=train, test=test)
}
split_predict = function(train, test) {
model = lm(attr_o ~ ., train)
pred_train = predict(model, train)
pred_test = predict(model, test)
list(train=pred_train, test=pred_test)
}
rmse = function(x, y) sqrt(mean((x-y)^2))
niter = 100
rmse_train = rep(0, niter)
rmse_test = rep(0, niter)
for (i in 1:niter) {
print(paste('Iteration:', i))
splits = split_data(df_attr)
preds = split_predict(splits$train, splits$test)
rmse_train[i] = rmse(preds$train, splits$train$attr_o)
rmse_test[i] = rmse(preds$test, splits$test$attr_o)
}
rmses = data.frame(rmse_train, rmse_test)
ggplot(rmses) + geom_histogram(aes(rmse_train), fill="blue", alpha=0.2) + geom_histogram(aes(rmse_test), fill="red", alpha=0.2)
sd(rmse_test)
sd(rmse_train)
# n-fold cross validation
nfold_cv = function(df, n_folds) {
# Make empty predictions vector
preds = numeric(nrow(df))
# Calculate the different folds
folds = sample(nrow(df)) %% n_folds + 1
for (i in 1:n_folds) {
# Get subsets of the data
df_train = df[folds != i, ]
df_test = df[folds == i, ]
# Fit linear model to training set
fit = lm(attr_o ~ ., df_train)
# Make predictions for test set
preds[folds == i] = predict(fit, df_test)
}
# Return RMSE
sqrt(mean((preds - df$attr_o)^2))
}
# Run 100 trials for calculation of RMSE
rmse_2 = numeric(100)
rmse_10 = numeric(100)
for (i in 1:100) {
print(paste('Iteration', i))
rmse_2[i] = nfold_cv(df_attr, 2)
rmse_10[i] = nfold_cv(df_attr, 10)
}
# Plot values of RMSE
df_rmse = data.frame(rmse_2, rmse_10)
ggplot(df_rmse) + geom_histogram(aes(x=rmse_2), alpha=0.3, fill="red") + geom_histogram(aes(x=rmse_10), alpha=0.3, fill="blue")
sd(rmse_2)
sd(rmse_10)
backward_step = function(df) {
rmse_cv = c()
rmse_nocv = c()
n_removed = c()
n_removed_count = 1
while (length(colnames(df)) > 1) {
# Fit model
model = lm(attr_o ~ ., df)
# Add cross-validated RMSE estimate
rmse_cv = c(rmse_cv, nfold_cv(df, 10))
# Add RMSE for linear regression on whole dataset
rmse_nocv = c(rmse_nocv, rmse(predict(model, df), df$attr_o))
# Add # features removed
n_removed = c(n_removed, n_removed_count)
# Feature elimination
sumDF = as.data.frame(summary(model)$coefficients)
sumDF = cbind(sumDF, rownames(sumDF))
sumDF = sumDF[-1, ] # remove Intercept row
maxRow = filter(sumDF, sumDF[4] == max(sumDF[4])) # biggest p-value
# Eliminate column
df = select(df, -one_of(as.character(maxRow[[5]])))
n_removed_count = n_removed_count + 1
}
data.frame(n_removed=n_removed, rmse_cv=rmse_cv, rmse_nocv=rmse_nocv)
}
step_results = backward_step(df_attr)
# Not a completely smooth plot, but RMSE decreases as
# features are removed and then sharply climbs when you've
# removed ~14 variables
ggplot(step_results) + geom_point(aes(x=n_removed, y=rmse_cv), color="red") + geom_point(aes(x=n_removed, y=rmse_nocv))
?one_of
?select
bootstrap_bad = function(df, approach) {
rmse_sum = 0
for (i in 1:100) {
idx_btstrap = sample(1:nrow(df), replace=TRUE)
df_btstrap = df[idx_btstrap, ]
if (approach == 1) {
train = df_btstrap
test = df
} else if (approach == 2) {
train = df
test = df_btstrap
}
# Train model
model = lm(attr_o ~ ., train)
preds = predict(model, test)
rmse_sum = rmse_sum + rmse(preds, test$attr_o)
}
rmse_sum / 100
}
backward_step_2 = function(df) {
rmse_cv = c()
rmse_nocv = c()
rmse_btstrap_1 = c()
rmse_btstrap_2 = c()
n_removed = c()
n_removed_count = 0
while (length(colnames(df)) > 1) {
# Fit model
model = lm(attr_o ~ ., df)
# Add cross-validated RMSE estimate
rmse_cv = c(rmse_cv, nfold_cv(df, 10))
# Add RMSE for linear regression on whole dataset
rmse_nocv = c(rmse_nocv, rmse(predict(model, df), df$attr_o))
# Add bootstrap RMSE
rmse_btstrap_1 = c(rmse_btstrap_1, bootstrap_bad(df, approach=1))
rmse_btstrap_2 = c(rmse_btstrap_2, bootstrap_bad(df, approach=2))
# Add # features removed
n_removed = c(n_removed, n_removed_count)
# Feature elimination
sumDF = as.data.frame(summary(model)$coefficients)
sumDF = cbind(sumDF, rownames(sumDF))
sumDF = sumDF[-1, ] # remove Intercept column
maxRow = filter(sumDF, sumDF[4] == max(sumDF[4])) # biggest p-value
# Eliminate column
df = select(df, -one_of(as.character(maxRow[[5]])))
n_removed_count = n_removed_count + 1
}
return(data.frame(n_removed=n_removed, rmse_cv=rmse_cv, rmse_nocv=rmse_nocv, rmse_btstrap_1=rmse_btstrap_1, rmse_btstrap_2=rmse_btstrap_2))
}
step_results_2 = backward_step_2(df_attr)
ggplot(step_results_2) + geom_point(aes(x=n_removed, y=rmse_cv), color="red") + geom_point(aes(x=n_removed, y=rmse_nocv)) + geom_point(aes(x=n_removed, y=rmse_btstrap_1), color="blue") + geom_point(aes(x=n_removed, y=rmse_btstrap_2), color="green")
calc_alpha = function(X, Y) {
sdX = sd(X)
sdY = sd(Y)
(sdY^2) / (sdX^2 + sdY^2)
}
gen_alphas = function(sdX, sdY) {
nobs = 100
X = rnorm(nobs, mean=10, sd=sdX)
Y = rnorm(nobs, mean=10, sd=sdY)
alphas = numeric(1000)
for (i in 1:1000) {
Xstrap = X[sample(1:nobs, replace=TRUE)]
Ystrap = Y[sample(1:nobs, replace=TRUE)]
alpha_est = calc_alpha(Xstrap, Ystrap)
alphas[i] = alpha_est
}
alphas
}
alph1 = gen_alphas(1, 3)
alph2 = gen_alphas(3, 1)
alph3 = gen_alphas(3, 3)
qplot(alph1)
qplot(alph2)
qplot(alph3)
qplot(alph3)
mean(alph1)
sd(alph1)
mean(alph2)
sd(alph2)
mean(alph3)
sd(alph3)
alph4 = gen_alphas(1, 2)
alph5 = gen_alphas(1, 3)
alph6 = gen_alphas(1, 4)
df_alph = data.frame(alph4, alph5, alph6)
ggplot(df_alph) + geom_histogram(aes(x=alph4), fill="red", alpha=0.2) + geom_histogram(aes(x=alph5), fill="blue", alpha=0.2) + geom_histogram(aes(x=alph6), fill="black", alpha=0.2)
set.seed(1); j = 50; a = 0.25
x = rnorm(j)
error = sqrt(1 - a^2)*rnorm(j)
y = a*x + error
# Look at lm() coefficient estimate
summary(lm(y ~ x - 1))
qplot(x, y) + geom_smooth(method = "lm")
# Define cost function
cost =  function(x, y, aEst, lambda, p){
sum((y - aEst*x)^2) + lambda*abs(aEst)^p
}
cost(1, 2, 3, 4, 2)
ambdas = 2^(seq(-2, 7, 1))
as = seq(-0.1, 0.3, by=0.001)
lambdas = 2^(seq(-2, 7, 1))
as = seq(-0.1, 0.3, by=0.001)
rid = expand.grid(lambda=lambdas, aEst=as)
# Add empty costL1 and costL2 columns
grid$costL1 = 0
grid$costL2 = 0
# Fill in empty columns
for (i in 1:nrow(grid)) {
aEst = grid[i, "aEst"]
lambda = grid[i, "lambda"]
grid[i, "costL1"] = cost(x, y, aEst, lambda, 1)
grid[i, "costL2"] = cost(x, y, aEst, lambda, 2)
}
# Create grid
grid = expand.grid(lambda=lambdas, aEst=as)
# Add empty costL1 and costL2 columns
grid$costL1 = 0
grid$costL2 = 0
for (i in 1:nrow(grid)) {
aEst = grid[i, "aEst"]
lambda = grid[i, "lambda"]
grid[i, "costL1"] = cost(x, y, aEst, lambda, 1)
grid[i, "costL2"] = cost(x, y, aEst, lambda, 2)
}
get_plot = function(lambda, p) {
aEst = grid[grid$lambda == lambda, "aEst"]
if (p == 1) {
cost = grid[grid$lambda == lambda, "costL1"]
} else {
cost = grid[grid$lambda == lambda, "costL2"]
}
qplot(aEst, cost)
}
# Make the lists of plots
plotsL1 = lapply(1:10, function(k) get_plot(lambdas[k], 1))
plotsL2 = lapply(1:10, function(k) get_plot(lambdas[k], 2))
# Calls to multiplot()
multiplot(plotlist=plotsL1, cols=2)
library(Rmisc)
# Calls to multiplot()
multiplot(plotlist=plotsL1, cols=2)
plotsL1
# Calls to multiplot()
multiplot(plotlist=plotsL1, cols=2)
)
multiplot(plotlist=plotsL2, cols=2)
df = read.csv('../../../datasets/speed-dating-simple.csv')
df = read.csv('../../../datasets/speed-dating-simple/speed-dating-simple.csv')
df = read.csv('../../datasets/speed-dating-simple/speed-dating-simple.csv')
# Filter for gender == 1
df = filter(df, gender==1)
# Get features/target variables
activities = select(df, sports:yoga)
activities_scaled = scale(activities)
attr_o = df$attr_o
# L1/L2 regularized fit on entire dataset
fit_l1 = glmnet(activities_scaled, attr_o, alpha=1)
fit_l2 = glmnet(activities_scaled, attr_o, alpha=0)
library(glmnet)
# L1/L2 regularized fit on entire dataset
fit_l1 = glmnet(activities_scaled, attr_o, alpha=1)
fit_l2 = glmnet(activities_scaled, attr_o, alpha=0)
fit_l1$lambda
fit_l2$lambda
get_rmses = function(fit, features, target) {
rmses = numeric(length(fit$lambda))
for (i in 1:length(fit$lambda)) {
lamb = fit$lambda[i]
preds = predict(fit, features, s=lamb)
tmp_rmse = rmse(preds, target)
rmses[i] = tmp_rmse
}
rmses
}
# Get RMSE values
rmse_l1 = get_rmses(fit_l1, activities_scaled, attr_o)
rmse_l2 = get_rmses(fit_l2, activities_scaled, attr_o)
# Plot RMSE against lambda
qplot(fit_l1$lambda, rmse_l1)
qplot(fit_l2$lambda, rmse_l2)
# Use cv.glmnet() to determine optimal cross-validated lambda
fit_l1_cv = cv.glmnet(activities_scaled, attr_o, alpha=1)
fit_l2_cv = cv.glmnet(activities_scaled, attr_o, alpha=0)
# Plot cross-validated error against lambda
qplot(fit_l1_cv$lambda, fit_l1_cv$cvm)
qplot(fit_l2_cv$lambda, fit_l2_cv$cvm)
df_step = select(df, attr_o, sports:yoga)
# Initialize predictions vectors
preds_step = numeric(nrow(df))
preds_l1 = numeric(nrow(df))
preds_l2 = numeric(nrow(df))
# Do cross-validation
n_folds = 20
folds = sample(nrow(df)) %% n_folds + 1
for (i in 1:n_folds) {
print(paste0("Running fold: ", as.character(i)))
# L1/L2 regressions
features_train = scale(activities[folds != i, ])
features_test = scale(activities[folds == i, ], center=attr(features_train, "scaled:center"), scale=attr(features_train, "scaled:scale"))
attr_o_train = attr_o[folds != i]
cv_fit_l1 = cv.glmnet(features_train, attr_o_train, alpha=1)
cv_fit_l2 = cv.glmnet(features_train, attr_o_train, alpha=0)
# Stepwise regression
df_step_train = df_step[folds != i, ]
df_step_test = df_step[folds == i, ]
model_init = lm(attr_o ~ ., df_step_train)
model_full = formula("attr_o ~ .")
#model_full = formula(model_init)
cv_fit_step = step(model_init, model_full, direction="backward", trace=0)
# Generate and store predictions
preds_step[folds == i] = predict(cv_fit_step, df_step_test)
preds_l1[folds == i] = predict(cv_fit_l1, features_test, s=cv_fit_l1$lambda.min)
preds_l2[folds == i] = predict(cv_fit_l2, features_test, s=cv_fit_l2$lambda.min)
}
# Calculate RMSE values
cv_rmse_step = rmse(preds_step, attr_o)
cv_rmse_l1 = rmse(preds_l1, attr_o)
cv_rmse_l2 = rmse(preds_l2, attr_o)
# Compare them
c(step=cv_rmse_step, l1=cv_rmse_l1, l2=cv_rmse_l2)
# Set parameters
param_grid = expand.grid(.alpha=1:10*0.1, .lambda=10^seq(-4, 0, length.out=10))
control = trainControl(method="repeatedcv", number=10, repeats=3, verboseIter=TRUE)
library(caret)
control = trainControl(method="repeatedcv", number=10, repeats=3, verboseIter=TRUE)
# Search
caret_fit = train(x=activities_scaled, y=attr_o, method="glmnet", tuneGrid=param_grid, trControl=control)
# View minimum RMSE
min(caret_fit$results$RMSE)
# View hyperparameters
caret_fit$bestTune
